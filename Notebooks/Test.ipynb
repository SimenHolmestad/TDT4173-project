{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 Machine learning model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.utils as ku \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check the number of lines in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataframe and balancing the data\n",
    "\n",
    "We want equal amounts of reviews for all ratings, so we set the `cap_number` as the minimum number of reviews for one rating. (this is the number of revies rated 2 stars)\n",
    "\n",
    "\n",
    "The process is done in a function for memory purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(filename):\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, engine=\"python\")\n",
    "    \n",
    "    unique, counts = np.unique(df[\"label\"], return_counts=True)\n",
    "    cap_number = min(counts)\n",
    "    \n",
    "    # Create one dataframe for reviews with each rating and sample `cap_number` rows for each.\n",
    "    dfs = []\n",
    "    for x in range(5):\n",
    "        # x_df = df[df[\"label\"]==x]\n",
    "        number_of_rows = len(df[df[\"label\"]==x].index)\n",
    "        n = min(cap_number, number_of_rows)\n",
    "        dfs.append(df[df[\"label\"]==x].sample(n=n))  # Sample chooses random rows\n",
    "        \n",
    "    # Return the concatinated dataframes in randomised order\n",
    "    return pd.concat(dfs).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataframe = create_balanced_dataset(\"output_final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688960\n"
     ]
    }
   ],
   "source": [
    "print(len(balanced_dataframe.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "data_file_path = \"output_final2.csv\"\n",
    "model_file_path = 'balanced_model_5epochs.h5'\n",
    "\n",
    "# The maximum number of words to be used, only most frequent\n",
    "vocabulary_size = 50000\n",
    "# Max number of words in each review\n",
    "max_review_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  training  label\n",
      "5878419  star received warm greeting offered glass wate...      0\n",
      "496230   way shape form impressed tasted like frozen cr...      0\n",
      "729601   candlewood great place stay rite strip importa...      3\n",
      "1479905  pollo loco love salads nights ago upset servic...      1\n",
      "579130   ordered drinks outside inside outside differen...      1\n",
      "Length of corpus: 2688960\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(data_file_path, error_bad_lines=False, engine=\"python\") # One of the lines (5945667) apparently contains an EOF-character\n",
    "\n",
    "df = balanced_dataframe\n",
    "print(df.head())\n",
    "print(\"Length of corpus:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 353815\n",
      "#unique tokens: 353814\n",
      "Shape of training data: (2688960, 100)\n",
      "Shape of label tensor: 2688960\n"
     ]
    }
   ],
   "source": [
    "# Get corpus\n",
    "corpus = df['training'].tolist()\n",
    "\n",
    "# Tokenize the corpus with the `vocabulary_size` most frequent words\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words:', total_words)\n",
    "print('#unique tokens:', total_words-1)\n",
    "\n",
    "# Create training vectors with padding, where applicable, is at the end \n",
    "X = tokenizer.texts_to_sequences(corpus)\n",
    "X = pad_sequences(X, maxlen=max_review_size)\n",
    "print('Shape of training data:', X.shape)\n",
    "\n",
    "# Get labels\n",
    "Y = df['label'].to_numpy()\n",
    "print('Shape of label tensor:', len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features:  (2420064, 100)\n",
      "Shape of training labels:  (2420064,)\n",
      "Shape of test features:  (268896, 100)\n",
      "Shape of test labels:  (268896,)\n"
     ]
    }
   ],
   "source": [
    "# Split features and labels into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(\"Shape of training features: \", X_train.shape)\n",
    "print(\"Shape of training labels: \", Y_train.shape)\n",
    "print(\"Shape of test features: \", X_test.shape)\n",
    "print(\"Shape of test labels: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 64, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(32, dropout=0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing model or train a new one\n",
    "\n",
    "# Define constants for training \n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1, verbose=0)\n",
    "model.save(model_file_path)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "accr = model.evaluate(X_test,Y_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
